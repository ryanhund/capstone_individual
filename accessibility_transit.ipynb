{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quiet-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')  # allows notebook to be tested in Travis\n",
    "\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandana as pdna\n",
    "import time\n",
    "\n",
    "import urbanaccess as ua\n",
    "from urbanaccess.config import settings\n",
    "from urbanaccess.gtfsfeeds import feeds\n",
    "from urbanaccess import gtfsfeeds\n",
    "from urbanaccess.gtfs.gtfsfeeds_dataframe import gtfsfeeds_dfs\n",
    "from urbanaccess.network import ua_network, load_network\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structured-jewelry",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Added 1 feeds to gtfs_feeds: {'metro_bus': 'https://gitlab.com/LACMTA/gtfs_bus/raw/master/gtfs_bus.zip'}\nAdded 1 feeds to gtfs_feeds: {'metro_rail': 'https://gitlab.com/LACMTA/gtfs_rail/raw/master/gtfs_rail.zip'}\n"
     ]
    }
   ],
   "source": [
    "feeds.add_feed(add_dict={'metro_bus': 'https://gitlab.com/LACMTA/gtfs_bus/raw/master/gtfs_bus.zip'})\n",
    "feeds.add_feed(add_dict={'metro_rail': 'https://gitlab.com/LACMTA/gtfs_rail/raw/master/gtfs_rail.zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-bones",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 GTFS feed(s) will be downloaded here: data\\gtfsfeed_zips\n",
      "metro_bus GTFS feed downloaded successfully. Took 6.25 seconds for 18,289,711.0KB\n",
      "metro_rail GTFS feed downloaded successfully. Took 0.82 seconds for 660,824.0KB\n",
      "GTFS feed download completed. Took 7.09 seconds\n",
      "metro_bus.zip successfully extracted to: data\\gtfsfeed_text\\metro_bus\n",
      "metro_rail.zip successfully extracted to: data\\gtfsfeed_text\\metro_rail\n",
      "GTFS feed zipfile extraction completed. Took 1.92 seconds for 2 files\n"
     ]
    }
   ],
   "source": [
    "gtfsfeeds.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparative-designation",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking GTFS text file header whitespace... Reading files using encoding: utf-8 set in configuration.\n",
      "GTFS text file header whitespace check completed. Took 0.85 seconds\n",
      "--------------------------------\n",
      "Processing GTFS feed: metro_bus\n",
      "The unique agency id: metro_-_los_angeles was generated using the name of the agency in the agency.txt file.\n",
      "Unique agency id operation complete. Took 0.09 seconds\n",
      "Unique GTFS feed id operation complete. Took 0.02 seconds\n",
      "WARNING: metro_bus GTFS feed stops: 1,762 of 13,283 (13.27 percent of total) record(s) are outside the bounding box coordinates\n",
      "Records:\n",
      "        stop_id  stop_code                    stop_name  stop_desc   stop_lat  \\\n",
      "20           33         33   Balboa / Foothill West Jog        NaN  34.322084   \n",
      "26           47         47                 7th / Maclay        NaN  34.293980   \n",
      "34           63         63            Agoura / La Venta        NaN  34.148950   \n",
      "35           65         65          Agoura / Lost Hills        NaN  34.139410   \n",
      "38           69         69    Lankershim / San Fernando        NaN  34.233180   \n",
      "...         ...        ...                          ...        ...        ...   \n",
      "12942    176144     176144            Tampa / Parthenia        NaN  34.228377   \n",
      "12943    176145     176145                Tampa / Chase        NaN  34.224742   \n",
      "13261  70020085   70020085        Agoura / Las Virgenes        NaN  34.143973   \n",
      "13262  70020246   70020246        Agoura / Las Virgenes        NaN  34.143801   \n",
      "13266  70500004   70500004  22718 Pacific Coast Highway        NaN  34.039323   \n",
      "\n",
      "         stop_lon  stop_url  location_type  parent_station  tpis_name  \\\n",
      "20    -118.495243       NaN            NaN             NaN        NaN   \n",
      "26    -118.427206       NaN            NaN             NaN        NaN   \n",
      "34    -118.822087       NaN            NaN             NaN        NaN   \n",
      "35    -118.708608       NaN            NaN             NaN        NaN   \n",
      "38    -118.387941       NaN            NaN             NaN        NaN   \n",
      "...           ...       ...            ...             ...        ...   \n",
      "12942 -118.553717       NaN            NaN             NaN        NaN   \n",
      "12943 -118.553703       NaN            NaN             NaN        NaN   \n",
      "13261 -118.698511       NaN            NaN             NaN        NaN   \n",
      "13262 -118.698717       NaN            NaN             NaN        NaN   \n",
      "13266 -118.669421       NaN            NaN             NaN        NaN   \n",
      "\n",
      "          unique_agency_id unique_feed_id  \n",
      "20     metro_-_los_angeles    metro_bus_1  \n",
      "26     metro_-_los_angeles    metro_bus_1  \n",
      "34     metro_-_los_angeles    metro_bus_1  \n",
      "35     metro_-_los_angeles    metro_bus_1  \n",
      "38     metro_-_los_angeles    metro_bus_1  \n",
      "...                    ...            ...  \n",
      "12942  metro_-_los_angeles    metro_bus_1  \n",
      "12943  metro_-_los_angeles    metro_bus_1  \n",
      "13261  metro_-_los_angeles    metro_bus_1  \n",
      "13262  metro_-_los_angeles    metro_bus_1  \n",
      "13266  metro_-_los_angeles    metro_bus_1  \n",
      "\n",
      "[1762 rows x 12 columns]\n",
      "Removed identified stops that are outside of bounding box.\n",
      "metro_bus GTFS feed stops: coordinates are in northwest hemisphere. Latitude = North (90); Longitude = West (-90).\n",
      "Appended route type to stops\n",
      "Appended route type to stop_times\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Processing GTFS feed: metro_rail\n",
      "data\\gtfsfeed_text\\metro_rail\\calendar_dates.txt has no records. This could indicate that this feed is using calendar.txt for service_ids.\n",
      "The unique agency id: metro_-_los_angeles was generated using the name of the agency in the agency.txt file.\n",
      "Unique agency id operation complete. Took 0.01 seconds\n",
      "Unique GTFS feed id operation complete. Took 0.00 seconds\n",
      "WARNING: metro_rail GTFS feed stops: 3 of 394 (0.76 percent of total) record(s) are outside the bounding box coordinates\n",
      "Records:\n",
      "    stop_id stop_code                                   stop_name  stop_desc  \\\n",
      "391   80427     80427                APU / Citrus College Station        NaN   \n",
      "392  80427S    80427S                APU / Citrus College Station        NaN   \n",
      "393  80427A    80427A  APU / Citrus College Station - East Portal        NaN   \n",
      "\n",
      "      stop_lat    stop_lon  stop_url  location_type parent_station tpis_name  \\\n",
      "391  34.136814 -117.891636       NaN              0         80427S     Azusa   \n",
      "392  34.136814 -117.891636       NaN              1            NaN       NaN   \n",
      "393  34.136940 -117.890850       NaN              2         80427S       NaN   \n",
      "\n",
      "        unique_agency_id unique_feed_id  \n",
      "391  metro_-_los_angeles   metro_rail_2  \n",
      "392  metro_-_los_angeles   metro_rail_2  \n",
      "393  metro_-_los_angeles   metro_rail_2  \n",
      "Removed identified stops that are outside of bounding box.\n",
      "metro_rail GTFS feed stops: coordinates are in northwest hemisphere. Latitude = North (90); Longitude = West (-90).\n",
      "Appended route type to stops\n",
      "Appended route type to stop_times\n",
      "--------------------------------\n",
      "Added descriptive definitions to stops, routes, stop_times, and trips tables\n",
      "Successfully converted ['departure_time'] to seconds past midnight and appended new columns to stop_times. Took 5.67 seconds\n",
      "2 GTFS feed file(s) successfully read as dataframes:\n",
      "     metro_bus\n",
      "     metro_rail\n",
      "     Took 14.68 seconds\n"
     ]
    }
   ],
   "source": [
    "validation = True\n",
    "verbose = True \n",
    "\n",
    "#bbox for project site\n",
    "bbox = -118.617668,33.660954,-117.900962,34.207872\n",
    "remove_stops_outsidebbox = True\n",
    "append_definitions = True\n",
    "\n",
    "loaded_feeds = ua.gtfs.load.gtfsfeed_to_df(gtfsfeed_path=None,\n",
    "                                           validation=validation,\n",
    "                                           verbose=verbose,\n",
    "                                           bbox=bbox,\n",
    "                                           remove_stops_outsidebbox=remove_stops_outsidebbox,\n",
    "                                           append_definitions=append_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prime-portugal",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using calendar to extract service_ids to select trips.\n",
      "23 service_ids were extracted from calendar\n",
      "14,084 trip(s) 41.47 percent of 33,959 total trip records were found in calendar for GTFS feed(s): ['metro bus', 'metro rail']\n",
      "NOTE: If you expected more trips to have been extracted and your GTFS feed(s) have a calendar_dates file, consider utilizing the calendar_dates_lookup parameter in order to add additional trips based on information inside of calendar_dates. This should only be done if you know the corresponding GTFS feed is using calendar_dates instead of calendar to specify service_ids. When in doubt do not use the calendar_dates_lookup parameter.\n",
      "14,084 of 33,959 total trips were extracted representing calendar day: monday. Took 0.14 seconds\n",
      "There are no departure time records missing from trips following the specified schedule. There are no records to interpolate.\n",
      "Difference between stop times has been successfully calculated. Took 4.41 seconds\n",
      "Stop times from 07:00:00 to 10:00:00 successfully selected 121,166 records out of 676,211 total records (17.92 percent of total). Took 0.18 seconds\n",
      "Starting transformation process for 3,241 total trips...\n",
      "stop time table transformation to Pandana format edge table completed. Took 7.89 seconds\n",
      "Time conversion completed: seconds converted to minutes.\n",
      "11,524 of 11,912 records selected from stops. Took 0.01 seconds\n",
      "stop time table transformation to Pandana format node table completed. Took 0.02 seconds\n",
      "route type successfully joined to transit edges. Took 4.34 seconds\n",
      "route id successfully joined to transit edges. Took 0.12 seconds\n",
      "Successfully created transit network. Took 20.17 seconds\n"
     ]
    }
   ],
   "source": [
    "#create network for weekday AM peak\n",
    "ua.gtfs.network.create_transit_net(gtfsfeeds_dfs=loaded_feeds,\n",
    "                                   day='monday',\n",
    "                                   timerange=['07:00:00', '10:00:00'],\n",
    "                                   calendar_dates_lookup=None)\n",
    "urbanaccess_ampeak = ua.network.ua_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instant-market",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requesting network data within bounding box from Overpass API in 4 request(s)\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.66095400,-118.61766800,33.93494309,-118.25646126);>;);out;'}\"\n",
      "Downloaded 29,255.0KB from www.overpass-api.de in 15.81 seconds\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.93076790,-118.61766800,34.20839551,-118.26046406);>;);out;'}\"\n",
      "Downloaded 73,849.6KB from www.overpass-api.de in 36.06 seconds\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.66095400,-118.26046406,33.93809888,-117.90095179);>;);out;'}\"\n",
      "Downloaded 0.7KB from www.overpass-api.de in 15.34 seconds\n",
      "Server at www.overpass-api.de returned status code 429 and no JSON data. Re-trying request in 126.00 seconds.\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.66095400,-118.26046406,33.93809888,-117.90095179);>;);out;'}\"\n",
      "Downloaded 77,380.6KB from www.overpass-api.de in 35.92 seconds\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.93494309,-118.26452115,34.20839551,-117.90095179);>;);out;'}\"\n",
      "Downloaded 0.7KB from www.overpass-api.de in 15.05 seconds\n",
      "Server at www.overpass-api.de returned status code 429 and no JSON data. Re-trying request in 61.00 seconds.\n",
      "Posting to http://www.overpass-api.de/api/interpreter with timeout=180, \"{'data': '[out:json][timeout:180];(way[\"highway\"][\"highway\"!~\"motor|proposed|construction|abandoned|platform|raceway\"][\"foot\"!~\"no\"][\"pedestrians\"!~\"no\"](33.93494309,-118.26452115,34.20839551,-117.90095179);>;);out;'}\"\n",
      "Downloaded 80,139.4KB from www.overpass-api.de in 39.52 seconds\n",
      "Downloaded OSM network data within bounding box from Overpass API in 4 request(s) and 353.20 seconds\n",
      "35,582 duplicate records removed. Took 24.64 seconds\n",
      "Returning OSM data with 1,561,519 nodes and 391,695 ways...\n",
      "Edge node pairs completed. Took 679.46 seconds\n",
      "Returning processed graph with 519,177 nodes and 1,470,854 edges...\n",
      "Completed OSM data download and Pandana node and edge table creation in 1,075.19 seconds\n",
      "Completed OSM data download and graph node and edge table creation in 1,075.39 seconds\n"
     ]
    }
   ],
   "source": [
    "#load pedestrian OSM data for LA basin (takes several minutes) (adding remove_lcn = True adds ~35 minutes)\n",
    "nodes, edges = ua.osm.load.ua_network_from_bbox(bbox=bbox, remove_lcn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dangerous-pilot",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created OSM network with travel time impedance using a travel speed of 3 MPH. Took 0.04 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<urbanaccess.network.urbanaccess_network at 0x270fd981748>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#create pedestrian-specific graph from OSM data\n",
    "ua.osm.network.create_osm_net(osm_edges=edges,\n",
    "                              osm_nodes=nodes,\n",
    "                              travel_speed_mph=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cultural-estonia",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded UrbanAccess network components comprised of:\n",
      "     Transit: 11,524 nodes and 117,925 edges;\n",
      "     OSM: 519,177 nodes and 1,470,854 edges\n",
      "Connector edges between the OSM and transit network nodes successfully completed. Took 9.35 seconds\n",
      "Edge and node tables formatted for Pandana with integer node ids: id_int, to_int, and from_int. Took 22.82 seconds\n",
      "Network edge and node network integration completed successfully resulting in a total of 530,701 nodes and 1,611,827 edges:\n",
      "     Transit: 11,524 nodes 117,925 edges;\n",
      "     OSM: 519,177 nodes 1,470,854 edges; and\n",
      "     OSM/Transit connector: 23,048 edges.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<urbanaccess.network.urbanaccess_network at 0x270fd981748>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#integrate OSM network with transit network\n",
    "ua.network.integrate_network(urbanaccess_network=urbanaccess_ampeak,\n",
    "                             headways=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "convinced-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to disk\n",
    "# ua.network.save_network(urbanaccess_network=urbanaccess_ampeak,\n",
    "                        # filename='am/ampeak_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "micro-project",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using calendar to extract service_ids to select trips.\n",
      "23 service_ids were extracted from calendar\n",
      "14,084 trip(s) 41.47 percent of 33,959 total trip records were found in calendar for GTFS feed(s): ['metro bus', 'metro rail']\n",
      "NOTE: If you expected more trips to have been extracted and your GTFS feed(s) have a calendar_dates file, consider utilizing the calendar_dates_lookup parameter in order to add additional trips based on information inside of calendar_dates. This should only be done if you know the corresponding GTFS feed is using calendar_dates instead of calendar to specify service_ids. When in doubt do not use the calendar_dates_lookup parameter.\n",
      "14,084 of 33,959 total trips were extracted representing calendar day: monday. Took 0.06 seconds\n",
      "There are no departure time records missing from trips following the specified schedule. There are no records to interpolate.\n",
      "Difference between stop times has been successfully calculated. Took 4.11 seconds\n",
      "Stop times from 11:00:00 to 14:00:00 successfully selected 113,371 records out of 676,211 total records (16.77 percent of total). Took 0.16 seconds\n",
      "Starting transformation process for 3,194 total trips...\n",
      "stop time table transformation to Pandana format edge table completed. Took 7.75 seconds\n",
      "Time conversion completed: seconds converted to minutes.\n",
      "11,272 of 11,912 records selected from stops. Took 0.02 seconds\n",
      "stop time table transformation to Pandana format node table completed. Took 0.01 seconds\n",
      "route type successfully joined to transit edges. Took 3.94 seconds\n",
      "route id successfully joined to transit edges. Took 0.09 seconds\n",
      "Successfully created transit network. Took 18.71 seconds\n"
     ]
    }
   ],
   "source": [
    "#repeat process for off-peak workday\n",
    "urbanaccess_midday = ua.gtfs.network.create_transit_net(gtfsfeeds_dfs=loaded_feeds,\n",
    "                                   day='monday',\n",
    "                                   timerange=['11:00:00', '14:00:00'],\n",
    "                                   calendar_dates_lookup=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "differential-horse",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded UrbanAccess network components comprised of:\n",
      "     Transit: 11,272 nodes and 110,177 edges;\n",
      "     OSM: 519,177 nodes and 1,470,854 edges\n",
      "Connector edges between the OSM and transit network nodes successfully completed. Took 7.32 seconds\n",
      "Edge and node tables formatted for Pandana with integer node ids: id_int, to_int, and from_int. Took 19.59 seconds\n",
      "Network edge and node network integration completed successfully resulting in a total of 530,449 nodes and 1,603,575 edges:\n",
      "     Transit: 11,272 nodes 110,177 edges;\n",
      "     OSM: 519,177 nodes 1,470,854 edges; and\n",
      "     OSM/Transit connector: 22,544 edges.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<urbanaccess.network.urbanaccess_network at 0x270fd981748>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ua.network.integrate_network(urbanaccess_network=urbanaccess_midday,\n",
    "                             headways=False)\n",
    "# ua.network.save_network(urbanaccess_network=urbanaccess_midday,\n",
    "                        # filename='midday_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stone-exchange",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using calendar to extract service_ids to select trips.\n",
      "17 service_ids were extracted from calendar\n",
      "9,473 trip(s) 27.90 percent of 33,959 total trip records were found in calendar for GTFS feed(s): ['metro bus', 'metro rail']\n",
      "NOTE: If you expected more trips to have been extracted and your GTFS feed(s) have a calendar_dates file, consider utilizing the calendar_dates_lookup parameter in order to add additional trips based on information inside of calendar_dates. This should only be done if you know the corresponding GTFS feed is using calendar_dates instead of calendar to specify service_ids. When in doubt do not use the calendar_dates_lookup parameter.\n",
      "9,473 of 33,959 total trips were extracted representing calendar day: sunday. Took 0.06 seconds\n",
      "There are no departure time records missing from trips following the specified schedule. There are no records to interpolate.\n",
      "Difference between stop times has been successfully calculated. Took 2.69 seconds\n",
      "Stop times from 11:00:00 to 14:00:00 successfully selected 91,476 records out of 509,931 total records (17.94 percent of total). Took 0.12 seconds\n",
      "Starting transformation process for 2,324 total trips...\n",
      "stop time table transformation to Pandana format edge table completed. Took 6.12 seconds\n",
      "Time conversion completed: seconds converted to minutes.\n",
      "10,327 of 11,912 records selected from stops. Took 0.02 seconds\n",
      "stop time table transformation to Pandana format node table completed. Took 0.01 seconds\n",
      "route type successfully joined to transit edges. Took 3.53 seconds\n",
      "route id successfully joined to transit edges. Took 0.07 seconds\n",
      "Successfully created transit network. Took 15.07 seconds\n",
      "Loaded UrbanAccess network components comprised of:\n",
      "     Transit: 10,327 nodes and 89,152 edges;\n",
      "     OSM: 519,177 nodes and 1,470,854 edges\n",
      "Connector edges between the OSM and transit network nodes successfully completed. Took 7.18 seconds\n",
      "Edge and node tables formatted for Pandana with integer node ids: id_int, to_int, and from_int. Took 19.33 seconds\n",
      "Network edge and node network integration completed successfully resulting in a total of 529,504 nodes and 1,580,660 edges:\n",
      "     Transit: 10,327 nodes 89,152 edges;\n",
      "     OSM: 519,177 nodes 1,470,854 edges; and\n",
      "     OSM/Transit connector: 20,654 edges.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<urbanaccess.network.urbanaccess_network at 0x270fd981748>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "urbanaccess_midday_weekend = ua.gtfs.network.create_transit_net(gtfsfeeds_dfs=loaded_feeds,\n",
    "                                   day='sunday',\n",
    "                                   timerange=['11:00:00', '14:00:00'],\n",
    "                                   calendar_dates_lookup=None)\n",
    "ua.network.integrate_network(urbanaccess_network=urbanaccess_midday_weekend,\n",
    "                             headways=False)\n",
    "# ua.network.save_network(urbanaccess_network=urbanaccess_midday_weekend,\n",
    "                        # filename='midday_weekend_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "material-senegal",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using calendar to extract service_ids to select trips.\n",
      "17 service_ids were extracted from calendar\n",
      "9,473 trip(s) 27.90 percent of 33,959 total trip records were found in calendar for GTFS feed(s): ['metro bus', 'metro rail']\n",
      "NOTE: If you expected more trips to have been extracted and your GTFS feed(s) have a calendar_dates file, consider utilizing the calendar_dates_lookup parameter in order to add additional trips based on information inside of calendar_dates. This should only be done if you know the corresponding GTFS feed is using calendar_dates instead of calendar to specify service_ids. When in doubt do not use the calendar_dates_lookup parameter.\n",
      "9,473 of 33,959 total trips were extracted representing calendar day: sunday. Took 0.06 seconds\n",
      "There are no departure time records missing from trips following the specified schedule. There are no records to interpolate.\n",
      "Difference between stop times has been successfully calculated. Took 2.70 seconds\n",
      "Stop times from 07:00:00 to 10:00:00 successfully selected 81,719 records out of 509,931 total records (16.03 percent of total). Took 0.12 seconds\n",
      "Starting transformation process for 1,968 total trips...\n",
      "stop time table transformation to Pandana format edge table completed. Took 4.86 seconds\n",
      "Time conversion completed: seconds converted to minutes.\n",
      "10,367 of 11,912 records selected from stops. Took 0.02 seconds\n",
      "stop time table transformation to Pandana format node table completed. Took 0.01 seconds\n",
      "route type successfully joined to transit edges. Took 3.34 seconds\n",
      "route id successfully joined to transit edges. Took 0.07 seconds\n",
      "Successfully created transit network. Took 13.65 seconds\n",
      "Loaded UrbanAccess network components comprised of:\n",
      "     Transit: 10,367 nodes and 79,751 edges;\n",
      "     OSM: 519,177 nodes and 1,470,854 edges\n",
      "Connector edges between the OSM and transit network nodes successfully completed. Took 7.28 seconds\n",
      "Edge and node tables formatted for Pandana with integer node ids: id_int, to_int, and from_int. Took 19.21 seconds\n",
      "Network edge and node network integration completed successfully resulting in a total of 529,544 nodes and 1,571,339 edges:\n",
      "     Transit: 10,367 nodes 79,751 edges;\n",
      "     OSM: 519,177 nodes 1,470,854 edges; and\n",
      "     OSM/Transit connector: 20,734 edges.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<urbanaccess.network.urbanaccess_network at 0x270fd981748>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "urbanaccess_ampeak_weekend = ua.gtfs.network.create_transit_net(gtfsfeeds_dfs=loaded_feeds,\n",
    "                                   day='sunday',\n",
    "                                   timerange=['07:00:00', '10:00:00'],\n",
    "                                   calendar_dates_lookup=None)\n",
    "ua.network.integrate_network(urbanaccess_network=urbanaccess_ampeak_weekend,\n",
    "                             headways=False)\n",
    "# ua.network.save_network(urbanaccess_network=urbanaccess_ampeak_weekend,\n",
    "                        # filename='ampeak_weekend_net.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "other-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handy-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd \n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "from descartes import PolygonPatch\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox \n",
    "\n",
    "trip_times = [15, 30, 45] #in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "varied-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def parse_nx_node(row, attr, idcol):\n",
    "    \"\"\"Parses nodes into nx-format\"\"\"\n",
    "    attr = row[attr].copy()\n",
    "    return (row[idcol], attr.to_dict())\n",
    "\n",
    "def parse_nx_edge(row, attr_cols, fr, to):\n",
    "    \"\"\"Parses edges into nx-format\"\"\"\n",
    "    idx = row.name\n",
    "    attr = row[attr_cols].copy().to_dict()\n",
    "    return (row[fr], row[to], idx, attr)\n",
    "\n",
    "\n",
    "def optimize_graph(edges, aggr_type, fr='from_int', to='to_int', weight_col='weight'):\n",
    "    \"\"\"\n",
    "    Optimize the graph in terms of keeping only e.g. 'min' or 'max' (fastest / slowest) edges between nodes.\n",
    "    This process can dramatically decrease the size of the graph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    edges : pd.DataFrame\n",
    "        Pandas DataFrame containing the edge information.\n",
    "    aggr_type : str\n",
    "        Aggregation type. Possible values are: 'min', 'max', 'mean', 'median'\n",
    "    \"\"\"\n",
    "    # Group and aggregate edges by from and to -nodes using selected aggregation method\n",
    "    optimized = edges.groupby([fr, to])[weight_col].agg(aggr_type).reset_index() \\\n",
    "        .merge(edges, on=[fr, to]).rename(columns={weight_col + '_x': weight_col}) \\\n",
    "        .drop_duplicates([fr, to]) \\\n",
    "        .drop(weight_col + '_y', axis=1)\n",
    "    return optimized\n",
    "\n",
    "\n",
    "def urbanaccess_to_nx(ua_G,\n",
    "                      edge_attr=['from', 'to', 'from_int', 'to_int', 'weight', 'net_type', 'route_type',\n",
    "                                 'sequence', 'unique_agency_id', 'unique_route_id',\n",
    "                                 'unique_trip_id'],\n",
    "                      crs={'init': 'epsg:4326'},\n",
    "                      minimize=False,\n",
    "                      use_integer_nodeids=True,\n",
    "                      optimize=None,\n",
    "                      weight_col='weight'\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Converts the input UrbanAccess graph to NetworkX MultiDiGraph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ua_G : <urbanaccess.network.urbanaccess_network>\n",
    "        UrbanAccess graph that should contain GTFS data as well as walking network, stored in net_edges & net_nodes.\n",
    "    edge_attr : list\n",
    "        Edge attributes that will be included in the final NetworkX graph. The necessary ones are 'from', 'to', and 'weight'.\n",
    "    crs : CRS dictionary.\n",
    "        Coordinate Reference System of the input data. GTFS and OSM are in WGS84 (epsg:4326) by default.\n",
    "    minimize : <boolean> (default: False)\n",
    "        If `minimize=True`, the tool will include only necessary attributes when creating the graph.\n",
    "    use_integer_nodeids: Boolean (default: True)\n",
    "        If `use_integer_nodeids=True`, the tool will build the graph using integer numbers as the nodeids.\n",
    "    optimize : str (default: None)\n",
    "        It is possible to optimize the network by including e.g. only fastest ('min') or longest ('max') journey legs between\n",
    "        stops that might vary at different times of the day due schedule changes. Possible aggregation methods are: None, 'min', 'max', 'mean', 'median'.\n",
    "    weight_col : str\n",
    "        Name of the column that contains the cost information.\n",
    "    Assumptions\n",
    "    -----------\n",
    "    The function assumes that the UrbanAccess graph is \"full\", i.e. the transit and walk networks have been built and integrated.\n",
    "    To check this, you should have data inside DataFrames under `ua.network.ua_network.net_edges`, `ua.network.ua_network.net_nodes`,\n",
    "    as well as in `ua.network.ua_network.net_connector_edges` (connects stops to walking network).\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform some basic sanity checks\n",
    "    assert isinstance(ua_G.net_nodes, pd.DataFrame), \"UrbanAccess network does not contain valid `net_nodes` DataFrame.\"\n",
    "    assert isinstance(ua_G.net_edges, pd.DataFrame), \"UrbanAccess network does not contain valid `net_edges` DataFrame.\"\n",
    "    assert len(ua_G.net_nodes) > 0, \"Could not find any nodes from UrbanAccess network.\"\n",
    "    assert len(ua_G.net_edges) > 0, \"Could not find any edges from UrbanAccess network.\"\n",
    "\n",
    "    print(\"Converting to NetworkX graph ..\")\n",
    "    log_start = time.time()\n",
    "    # Connected graph items\n",
    "    nodes = ua_G.net_nodes\n",
    "    edges = ua_G.net_edges\n",
    "    graph = nx.MultiDiGraph()\n",
    "\n",
    "    # Reset index for nodes to get the integer version of the node_id\n",
    "    nodes = nodes.reset_index()\n",
    "\n",
    "    if use_integer_nodeids:\n",
    "        fr = 'from_int'\n",
    "        to = 'to_int'\n",
    "        nodeid = 'id_int'\n",
    "    else:\n",
    "        fr = 'from'\n",
    "        to = 'to'\n",
    "        nodeid = 'id'\n",
    "\n",
    "    if minimize:\n",
    "        exp_edge_attr = [fr, to, weight_col]\n",
    "        exp_node_attr = [nodeid, 'x', 'y']\n",
    "    else:\n",
    "        exp_edge_attr = edge_attr\n",
    "        exp_node_attr = list(nodes.columns)\n",
    "\n",
    "    if optimize is not None:\n",
    "        assert optimize in ['min', 'max', 'mean', 'median'], \"Possible optimization methods are: 'min', 'max', 'mean', 'median'. Got %s.\" % optimize\n",
    "        edges = optimize_graph(edges=edges, aggr_type=optimize, weight_col=weight_col)\n",
    "\n",
    "    # Convert nodes\n",
    "    start = time.time()\n",
    "    nodes['nx_node'] = nodes.apply(parse_nx_node, attr=exp_node_attr, idcol=nodeid, axis=1)\n",
    "    print(\"Converted nodes in %s minutes.\" % (round((time.time() - start) / 60, 1)))\n",
    "\n",
    "    # Convert edges\n",
    "    start = time.time()\n",
    "    edges['nx_edge'] = edges.apply(parse_nx_edge, attr_cols=exp_edge_attr, fr=fr, to=to, axis=1)\n",
    "    print(\"Converted edges in %s minutes.\" % (round((time.time() - start) / 60, 1)))\n",
    "\n",
    "    # Create the Nx Graph\n",
    "    graph.add_nodes_from(nodes['nx_node'].to_list())\n",
    "    graph.add_edges_from(edges['nx_edge'].to_list())\n",
    "\n",
    "    # Add metadata so that the graph works directly e.g. with OSMnx functions\n",
    "    graph.graph['crs'] = crs\n",
    "    graph.graph['name'] = edges['unique_agency_id'].unique()[0]\n",
    "    print(\"Created NetworkX graph in %s minutes with %s nodes and %s edges.\" % (\n",
    "    round((time.time() - log_start) / 60, 1), len(nodes), len(edges)))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "orange-poster",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converting to NetworkX graph ..\n",
      "Converted nodes in 5.7 minutes.\n",
      "Converted edges in 18.4 minutes.\n",
      "Created NetworkX graph in 24.5 minutes with 529544 nodes and 1571339 edges.\n"
     ]
    }
   ],
   "source": [
    "G = urbanaccess_to_nx(urbanaccess_ampeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "documented-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_site = (33.9379368079315, -118.25653793837309)\n",
    "\n",
    "center_node = ox.distance.get_nearest_node(G, project_site, method='haversine', return_dist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adverse-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "mile_time = 0 #correct for mile circle\n",
    "\n",
    "isochrone_polys = []\n",
    "for trip_time in sorted(trip_times, reverse=True):\n",
    "    subgraph = nx.ego_graph(G, center_node, radius=trip_time + mile_time, distance='weight')\n",
    "    node_points = [Point((data['x'], data['y'])) for node, data in subgraph.nodes(data=True)]\n",
    "    bounding_poly = gpd.GeoSeries(node_points).unary_union.convex_hull\n",
    "    isochrone_polys.append(bounding_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'init': 'epsg:4326'}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "G.graph['crs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import pyproj\n",
    "from shapely import geometry\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "\n",
    "lon, lat = 33.93790489175171, -118.25653601748706 # lon lat for Lanzit site\n",
    "radius = 1609  # in m\n",
    "\n",
    "# local_azimuthal_projection = \"+proj=aeqd +R=6371000 +units=m +lat_0={} +lon_0={}\".format(\n",
    "#     lat, lon\n",
    "# )\n",
    "local_azimuthal_projection = 'epsg:4326'\n",
    "wgs84_to_aeqd = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    "    pyproj.Proj(local_azimuthal_projection),\n",
    ")\n",
    "aeqd_to_wgs84 = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj(local_azimuthal_projection),\n",
    "    pyproj.Proj(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    ")\n",
    "\n",
    "center = Point(float(lon), float(lat))\n",
    "point_transformed = transform(wgs84_to_aeqd, center)\n",
    "buffer = point_transformed.buffer(radius)\n",
    "# Get the polygon with lat lon coordinates\n",
    "circle_poly = transform(aeqd_to_wgs84, buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one color for each isochrone\n",
    "iso_colors = ox.plot.get_colors(n=len(trip_times), cmap='plasma', start=0, return_hex=True)\n",
    "\n",
    "# plot the network then add isochrones as colored descartes polygon patches\n",
    "fig, ax = ox.plot_graph(G, show=False, close=False, edge_color='#999999', edge_alpha=0.2, node_size=0, figsize=(12,8))\n",
    "for polygon, fc in zip(isochrone_polys, iso_colors):\n",
    "    patch = PolygonPatch(polygon, fc=fc, ec='none', alpha=0.95, zorder=-1)\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "# ax.add_patch(plt.Circle(center_UTM,1609,fill=None,ec='mediumpurple'))\n",
    "ax.add_patch(PolygonPatch(circle_poly, fill=None,ec='mediumpurple'))\n",
    "\n",
    "plt.savefig('figures/ampeak.png', bbox_inches='tight', dpi=300)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "similar-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona \n",
    "\n",
    "# Enable fiona driver\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "chronic-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = gpd.GeoSeries(isochrone_polys)\n",
    "gds.to_file('transit_isochrone.kml',driver='KML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "useful-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_key(dictionary):\n",
    "    for key in dictionary:\n",
    "        return key\n",
    "    raise IndexError\n",
    "\n",
    "def make_iso_polys(G, edge_buff=25, node_buff=50, infill=False):\n",
    "    isochrone_polys = []\n",
    "    for trip_time in sorted(trip_times, reverse=True):\n",
    "        subgraph = nx.ego_graph(G, center_node, radius=trip_time, distance='weight')\n",
    "\n",
    "        node_points = [Point((data['x'], data['y'])) for node, data in subgraph.nodes(data=True)]\n",
    "        nodes_gdf = gpd.GeoDataFrame({'id': list(subgraph.nodes)}, geometry=node_points)\n",
    "        nodes_gdf = nodes_gdf.set_index('id')\n",
    "\n",
    "        edge_lines = []\n",
    "        for n_fr, n_to in subgraph.edges():\n",
    "            f = nodes_gdf.loc[n_fr].geometry\n",
    "            t = nodes_gdf.loc[n_to].geometry\n",
    "            first = get_first_key(G.get_edge_data(n_fr, n_to))\n",
    "            edge_lookup = G.get_edge_data(n_fr, n_to)[first].get('geometry',  LineString([f,t]))\n",
    "            edge_lines.append(edge_lookup)\n",
    "\n",
    "        n = nodes_gdf.buffer(node_buff).geometry\n",
    "        e = gpd.GeoSeries(edge_lines).buffer(edge_buff).geometry\n",
    "        all_gs = list(n) + list(e)\n",
    "        new_iso = gpd.GeoSeries(all_gs).unary_union\n",
    "        \n",
    "        # try to fill in surrounded areas so shapes will appear solid and blocks without white space inside them\n",
    "        if infill:\n",
    "            new_iso = Polygon(new_iso.exterior)\n",
    "        isochrone_polys.append(new_iso)\n",
    "    return isochrone_polys\n",
    "\n",
    "# isochrone_polys_buffer = make_iso_polys(G, edge_buff=25, node_buff=50, infill=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (final)",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}